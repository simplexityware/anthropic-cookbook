{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Configure Gemini API with your API key (as environment variable, recommended)\n",
        "GOOGLE_API_KEY = \"AIzaSyCwHlMwK5dKgPuQNS4_cIDniIbocITDsbs\"\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "def llm_call(prompt: str, system_prompt: str = \"\", model_name=\"gemini-pro\", temperature=0.1, max_tokens=4096) -> str:\n",
        "    \"\"\"\n",
        "    Calls the Gemini model with the given prompt and returns the response.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The user prompt to send to the model.\n",
        "        system_prompt (str, optional): The system prompt to send to the model. Defaults to \"\".\n",
        "        model_name (str, optional): The model to use for the call. Defaults to \"gemini-pro\".\n",
        "        temperature (float, optional): the temperature of the model. Default to 0.1\n",
        "        max_tokens (int, optional): The maximum number of tokens to generate. Defaults to 4096.\n",
        "\n",
        "    Returns:\n",
        "        str: The response from the language model.\n",
        "    \"\"\"\n",
        "    model = genai.GenerativeModel(model_name)\n",
        "    messages = []\n",
        "    if system_prompt:\n",
        "        messages.append({\"role\": \"user\", \"parts\": [system_prompt]}) # System prompt is added as a user prompt\n",
        "    messages.append({\"role\": \"user\", \"parts\": [prompt]})\n",
        "\n",
        "    try:\n",
        "        chat = model.start_chat(history=messages)\n",
        "        response = chat.send_message(messages[-1]['parts'], generation_config=genai.types.GenerationConfig(temperature=temperature, max_output_tokens=max_tokens))\n",
        "\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error communicating with Gemini API: {e}\")\n",
        "        return \"An error occurred while communicating with the Gemini API.\"\n",
        "\n",
        "\n",
        "def extract_xml(text: str, tag: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts the content of the specified XML tag from the given text. Used for parsing structured responses.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text containing the XML.\n",
        "        tag (str): The XML tag to extract content from.\n",
        "\n",
        "    Returns:\n",
        "        str: The content of the specified XML tag, or an empty string if the tag is not found.\n",
        "    \"\"\"\n",
        "    match = re.search(f'<{tag}>(.*?)</{tag}>', text, re.DOTALL)\n",
        "    return match.group(1) if match else \"\"\n",
        "\n",
        "# Example Usage\n",
        "example_prompt = \"Please output the following in XML with tag <example>My response here</example>.\"\n",
        "response_from_llm = llm_call(example_prompt)\n",
        "print(response_from_llm)\n",
        "extracted_text = extract_xml(response_from_llm, \"example\")\n",
        "print(extracted_text)"
      ],
      "metadata": {
        "id": "VkdTiJLivbxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQwfhdrrvSJk"
      },
      "source": [
        "## Evaluator-Optimizer Workflow\n",
        "In this workflow, one LLM call generates a response while another provides evaluation and feedback in a loop.\n",
        "\n",
        "### When to use this workflow\n",
        "This workflow is particularly effective when we have:\n",
        "\n",
        "- Clear evaluation criteria\n",
        "- Value from iterative refinement\n",
        "\n",
        "The two signs of good fit are:\n",
        "\n",
        "- LLM responses can be demonstrably improved when feedback is provided\n",
        "- The LLM can provide meaningful feedback itself"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixePgi9cvSJr"
      },
      "outputs": [],
      "source": [
        "from util import llm_call, extract_xml\n",
        "\n",
        "def generate(prompt: str, task: str, context: str = \"\") -> tuple[str, str]:\n",
        "    \"\"\"Generate and improve a solution based on feedback.\"\"\"\n",
        "    full_prompt = f\"{prompt}\\n{context}\\nTask: {task}\" if context else f\"{prompt}\\nTask: {task}\"\n",
        "    response = llm_call(full_prompt)\n",
        "    thoughts = extract_xml(response, \"thoughts\")\n",
        "    result = extract_xml(response, \"response\")\n",
        "\n",
        "    print(\"\\n=== GENERATION START ===\")\n",
        "    print(f\"Thoughts:\\n{thoughts}\\n\")\n",
        "    print(f\"Generated:\\n{result}\")\n",
        "    print(\"=== GENERATION END ===\\n\")\n",
        "\n",
        "    return thoughts, result\n",
        "\n",
        "def evaluate(prompt: str, content: str, task: str) -> tuple[str, str]:\n",
        "    \"\"\"Evaluate if a solution meets requirements.\"\"\"\n",
        "    full_prompt = f\"{prompt}\\nOriginal task: {task}\\nContent to evaluate: {content}\"\n",
        "    response = llm_call(full_prompt)\n",
        "    evaluation = extract_xml(response, \"evaluation\")\n",
        "    feedback = extract_xml(response, \"feedback\")\n",
        "\n",
        "    print(\"=== EVALUATION START ===\")\n",
        "    print(f\"Status: {evaluation}\")\n",
        "    print(f\"Feedback: {feedback}\")\n",
        "    print(\"=== EVALUATION END ===\\n\")\n",
        "\n",
        "    return evaluation, feedback\n",
        "\n",
        "def loop(task: str, evaluator_prompt: str, generator_prompt: str) -> tuple[str, list[dict]]:\n",
        "    \"\"\"Keep generating and evaluating until requirements are met.\"\"\"\n",
        "    memory = []\n",
        "    chain_of_thought = []\n",
        "\n",
        "    thoughts, result = generate(generator_prompt, task)\n",
        "    memory.append(result)\n",
        "    chain_of_thought.append({\"thoughts\": thoughts, \"result\": result})\n",
        "\n",
        "    while True:\n",
        "        evaluation, feedback = evaluate(evaluator_prompt, result, task)\n",
        "        if evaluation == \"PASS\":\n",
        "            return result, chain_of_thought\n",
        "\n",
        "        context = \"\\n\".join([\n",
        "            \"Previous attempts:\",\n",
        "            *[f\"- {m}\" for m in memory],\n",
        "            f\"\\nFeedback: {feedback}\"\n",
        "        ])\n",
        "\n",
        "        thoughts, result = generate(generator_prompt, task, context)\n",
        "        memory.append(result)\n",
        "        chain_of_thought.append({\"thoughts\": thoughts, \"result\": result})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0aFOMScvSJv"
      },
      "source": [
        "### Example Use Case: Iterative coding loop\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsfNlppMvSJw",
        "outputId": "880a17b7-0e01-4b4b-bd8f-fa5ea2a0bf8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== GENERATION START ===\n",
            "Thoughts:\n",
            "\n",
            "The task requires implementing a Stack with constant time operations including finding minimum. \n",
            "To achieve O(1) for getMin(), we need to maintain a second stack that keeps track of minimums.\n",
            "Each time we push, if the value is smaller than current min, we add it to minStack.\n",
            "When we pop, if the popped value equals current min, we also pop from minStack.\n",
            "\n",
            "\n",
            "Generated:\n",
            "\n",
            "```python\n",
            "class MinStack:\n",
            "    def __init__(self):\n",
            "        self.stack = []\n",
            "        self.minStack = []\n",
            "        \n",
            "    def push(self, x: int) -> None:\n",
            "        self.stack.append(x)\n",
            "        if not self.minStack or x <= self.minStack[-1]:\n",
            "            self.minStack.append(x)\n",
            "            \n",
            "    def pop(self) -> None:\n",
            "        if not self.stack:\n",
            "            return\n",
            "        if self.stack[-1] == self.minStack[-1]:\n",
            "            self.minStack.pop()\n",
            "        self.stack.pop()\n",
            "        \n",
            "    def getMin(self) -> int:\n",
            "        if not self.minStack:\n",
            "            return None\n",
            "        return self.minStack[-1]\n",
            "```\n",
            "\n",
            "=== GENERATION END ===\n",
            "\n",
            "=== EVALUATION START ===\n",
            "Status: NEEDS_IMPROVEMENT\n",
            "Feedback: \n",
            "While the implementation is generally correct and achieves O(1) time complexity for all operations, there are several areas for improvement:\n",
            "\n",
            "1. Error Handling:\n",
            "- pop() should raise an exception when stack is empty rather than silently returning\n",
            "- getMin() should raise an exception when stack is empty rather than returning None\n",
            "- These behaviors should be consistent with standard stack implementations\n",
            "\n",
            "2. Type Hints:\n",
            "- Return type hint for pop() should be None or void\n",
            "- Missing type hints for class variables stack and minStack\n",
            "\n",
            "3. Documentation:\n",
            "- Missing docstrings for class and methods\n",
            "- Missing parameter descriptions\n",
            "\n",
            "4. Edge Cases:\n",
            "- No explicit handling of invalid input types for push()\n",
            "\n",
            "The core algorithm is correct and efficient, but the implementation could be more robust and better documented for production use.\n",
            "\n",
            "=== EVALUATION END ===\n",
            "\n",
            "\n",
            "=== GENERATION START ===\n",
            "Thoughts:\n",
            "\n",
            "Based on the feedback, I'll improve the implementation by:\n",
            "1. Adding proper error handling with exceptions\n",
            "2. Including type hints and docstrings\n",
            "3. Adding input validation\n",
            "4. Maintaining O(1) time complexity for all operations\n",
            "\n",
            "\n",
            "Generated:\n",
            "\n",
            "```python\n",
            "from typing import List\n",
            "\n",
            "class MinStack:\n",
            "    \"\"\"A stack that supports push, pop, and getting minimum element in O(1) time.\"\"\"\n",
            "    \n",
            "    def __init__(self):\n",
            "        \"\"\"Initialize empty stack with two internal lists.\"\"\"\n",
            "        self.stack: List[int] = []\n",
            "        self.minStack: List[int] = []\n",
            "        \n",
            "    def push(self, x: int) -> None:\n",
            "        \"\"\"\n",
            "        Push element onto stack and update minimum stack.\n",
            "        \n",
            "        Args:\n",
            "            x: Integer to push onto stack\n",
            "            \n",
            "        Raises:\n",
            "            TypeError: If x is not an integer\n",
            "        \"\"\"\n",
            "        if not isinstance(x, int):\n",
            "            raise TypeError(\"Input must be an integer\")\n",
            "            \n",
            "        self.stack.append(x)\n",
            "        if not self.minStack or x <= self.minStack[-1]:\n",
            "            self.minStack.append(x)\n",
            "            \n",
            "    def pop(self) -> None:\n",
            "        \"\"\"\n",
            "        Remove and return top element from stack.\n",
            "        \n",
            "        Raises:\n",
            "            IndexError: If stack is empty\n",
            "        \"\"\"\n",
            "        if not self.stack:\n",
            "            raise IndexError(\"Cannot pop from empty stack\")\n",
            "            \n",
            "        if self.stack[-1] == self.minStack[-1]:\n",
            "            self.minStack.pop()\n",
            "        self.stack.pop()\n",
            "        \n",
            "    def getMin(self) -> int:\n",
            "        \"\"\"\n",
            "        Get minimum element in stack.\n",
            "        \n",
            "        Returns:\n",
            "            int: Minimum element in stack\n",
            "            \n",
            "        Raises:\n",
            "            IndexError: If stack is empty\n",
            "        \"\"\"\n",
            "        if not self.minStack:\n",
            "            raise IndexError(\"Cannot get minimum from empty stack\")\n",
            "        return self.minStack[-1]\n",
            "```\n",
            "\n",
            "=== GENERATION END ===\n",
            "\n"
          ]
        }
      ],
      "source": [
        "evaluator_prompt = \"\"\"\n",
        "Evaluate this following code implementation for:\n",
        "1. code correctness\n",
        "2. time complexity\n",
        "3. style and best practices\n",
        "\n",
        "You should be evaluating only and not attemping to solve the task.\n",
        "Only output \"PASS\" if all criteria are met and you have no further suggestions for improvements.\n",
        "Output your evaluation concisely in the following format.\n",
        "\n",
        "<evaluation>PASS, NEEDS_IMPROVEMENT, or FAIL</evaluation>\n",
        "<feedback>\n",
        "What needs improvement and why.\n",
        "</feedback>\n",
        "\"\"\"\n",
        "\n",
        "generator_prompt = \"\"\"\n",
        "Your goal is to complete the task based on <user input>. If there are feedback\n",
        "from your previous generations, you should reflect on them to improve your solution\n",
        "\n",
        "Output your answer concisely in the following format:\n",
        "\n",
        "<thoughts>\n",
        "[Your understanding of the task and feedback and how you plan to improve]\n",
        "</thoughts>\n",
        "\n",
        "<response>\n",
        "[Your code implementation here]\n",
        "</response>\n",
        "\"\"\"\n",
        "\n",
        "task = \"\"\"\n",
        "<user input>\n",
        "Implement a Stack with:\n",
        "1. push(x)\n",
        "2. pop()\n",
        "3. getMin()\n",
        "All operations should be O(1).\n",
        "</user input>\n",
        "\"\"\"\n",
        "\n",
        "loop(task, evaluator_prompt, generator_prompt)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}